{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cost estimator calibration\n",
        "\n",
        "This notebook gathers runtime measurements for QuASAr backends and conversion paths, fits the\n",
        "parameters used by :class:`quasar.cost_estimator.CostEstimator`, and reports calibration metrics.\n",
        "It exports the fitted model as JSON together with per-sample predictions for traceability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Requirements\n",
        "\n",
        "Install the simulation dependencies listed in `requirements.txt`. The notebook also expects\n",
        "`pandas` and `scikit-learn`-style numerical tools (NumPy and Matplotlib are already required by\n",
        "the project).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from typing import Dict, Iterable, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "try:\n",
        "    import pandas as pd\n",
        "except ImportError:  # pragma: no cover - optional dependency\n",
        "    pd = None\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "REPO_ROOT = Path.cwd()\n",
        "if (REPO_ROOT / 'scripts').exists() and str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(REPO_ROOT))\n",
        "\n",
        "from qiskit.quantum_info import Clifford\n",
        "\n",
        "from scripts.calibration.common import (\n",
        "    build_clifford_tail,\n",
        "    collect_metrics,\n",
        "    split_at_first_nonclifford,\n",
        "    count_ops,\n",
        ")\n",
        "from quasar.backends.sv import StatevectorBackend\n",
        "from quasar.backends.dd import DecisionDiagramBackend, ddsim_available\n",
        "from quasar.backends.tableau import TableauBackend, stim_available\n",
        "from quasar.conversion.tab2sv import tableau_to_statevector\n",
        "from quasar.cost_estimator import CostEstimator, CostParams\n",
        "\n",
        "plt.rcParams.update({'figure.figsize': (7.5, 4.5), 'axes.grid': True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class SampleSpec:\n",
        "    n: int\n",
        "    depth_cliff: int\n",
        "    tail_layers: int\n",
        "    angle_scale: float\n",
        "    seed: int\n",
        "\n",
        "\n",
        "def time_call(func, *args, **kwargs):\n",
        "    start = time.perf_counter()\n",
        "    error = None\n",
        "    value = None\n",
        "    try:\n",
        "        value = func(*args, **kwargs)\n",
        "        success = value is not None\n",
        "    except Exception as exc:  # pragma: no cover - runtime dependency guard\n",
        "        error = repr(exc)\n",
        "        success = False\n",
        "    elapsed = time.perf_counter() - start\n",
        "    return elapsed, success, value, error\n",
        "\n",
        "\n",
        "def generate_specs(\n",
        "    *,\n",
        "    qubits: Iterable[int],\n",
        "    samples_per_setting: int,\n",
        "    depth_cliff: int,\n",
        "    tail_layers: int,\n",
        "    angle_scale: float,\n",
        "    seed: int,\n",
        ") -> List[SampleSpec]:\n",
        "    specs: List[SampleSpec] = []\n",
        "    for index, n in enumerate(qubits):\n",
        "        base_seed = seed + 31 * index\n",
        "        for offset in range(samples_per_setting):\n",
        "            specs.append(\n",
        "                SampleSpec(\n",
        "                    n=int(n),\n",
        "                    depth_cliff=int(depth_cliff),\n",
        "                    tail_layers=int(tail_layers),\n",
        "                    angle_scale=float(angle_scale),\n",
        "                    seed=base_seed + 17 * offset,\n",
        "                )\n",
        "            )\n",
        "    return specs\n",
        "\n",
        "\n",
        "def collect_calibration_samples(specs: Iterable[SampleSpec]) -> List[Dict[str, object]]:\n",
        "    tableau = TableauBackend() if stim_available() else None\n",
        "    sv_backend = StatevectorBackend()\n",
        "    dd_backend = DecisionDiagramBackend() if ddsim_available() else None\n",
        "\n",
        "    records: List[Dict[str, object]] = []\n",
        "    for sample_id, spec in enumerate(specs):\n",
        "        circuit = build_clifford_tail(\n",
        "            n=spec.n,\n",
        "            depth_cliff=spec.depth_cliff,\n",
        "            tail_layers=spec.tail_layers,\n",
        "            angle_scale=spec.angle_scale,\n",
        "            seed=spec.seed,\n",
        "        )\n",
        "        metrics_full = collect_metrics(circuit)\n",
        "        one_full, two_full = count_ops(circuit.data)\n",
        "        base = {\n",
        "            'sample_id': sample_id,\n",
        "            'num_qubits': int(metrics_full.get('num_qubits', spec.n)),\n",
        "            'num_gates': int(metrics_full.get('num_gates', one_full + two_full)),\n",
        "            'one_qubit_gates': int(one_full),\n",
        "            'two_qubit_gates': int(two_full),\n",
        "            'rotation_count': int(metrics_full.get('rotation_count', 0)),\n",
        "            'sparsity': float(metrics_full.get('sparsity', 0.0) or 0.0),\n",
        "        }\n",
        "\n",
        "        elapsed, success, _, error = time_call(sv_backend.run, circuit, want_statevector=True)\n",
        "        records.append({**base, 'backend': 'sv', 'component': 'full', 'elapsed_s': elapsed, 'success': success, 'error': error})\n",
        "\n",
        "        if dd_backend is not None:\n",
        "            elapsed, success, _, error = time_call(dd_backend.run, circuit, want_statevector=False)\n",
        "            records.append({**base, 'backend': 'dd', 'component': 'full', 'elapsed_s': elapsed, 'success': success, 'error': error})\n",
        "\n",
        "        split = split_at_first_nonclifford(circuit)\n",
        "        if split is None:\n",
        "            continue\n",
        "\n",
        "        prefix_metrics = collect_metrics(split.prefix)\n",
        "        tail_metrics = collect_metrics(split.tail)\n",
        "        one_pre, two_pre = count_ops(split.prefix.data)\n",
        "        one_tail, two_tail = count_ops(split.tail.data)\n",
        "\n",
        "        prefix_base = {\n",
        "            'sample_id': sample_id,\n",
        "            'num_qubits': int(prefix_metrics.get('num_qubits', spec.n)),\n",
        "            'num_gates': int(prefix_metrics.get('num_gates', one_pre + two_pre)),\n",
        "            'one_qubit_gates': int(one_pre),\n",
        "            'two_qubit_gates': int(two_pre),\n",
        "            'rotation_count': int(prefix_metrics.get('rotation_count', 0)),\n",
        "            'sparsity': float(prefix_metrics.get('sparsity', 0.0) or 0.0),\n",
        "        }\n",
        "        tail_base = {\n",
        "            'sample_id': sample_id,\n",
        "            'num_qubits': int(tail_metrics.get('num_qubits', spec.n)),\n",
        "            'num_gates': int(tail_metrics.get('num_gates', one_tail + two_tail)),\n",
        "            'one_qubit_gates': int(one_tail),\n",
        "            'two_qubit_gates': int(two_tail),\n",
        "            'rotation_count': int(tail_metrics.get('rotation_count', 0)),\n",
        "            'sparsity': float(tail_metrics.get('sparsity', 0.0) or 0.0),\n",
        "        }\n",
        "\n",
        "        prefix_state = None\n",
        "        if tableau is not None:\n",
        "            elapsed, success, prefix_state, error = time_call(tableau.run, split.prefix, want_statevector=True)\n",
        "            records.append({**prefix_base, 'backend': 'tableau', 'component': 'prefix', 'elapsed_s': elapsed, 'success': success, 'error': error})\n",
        "        if prefix_state is None:\n",
        "            try:\n",
        "                prefix_state = tableau_to_statevector(Clifford(split.prefix))\n",
        "            except Exception as exc:  # pragma: no cover - runtime dependency guard\n",
        "                prefix_state = None\n",
        "                records.append({**prefix_base, 'backend': 'conversion', 'component': 'conversion', 'elapsed_s': None, 'success': False, 'error': repr(exc)})\n",
        "        else:\n",
        "            elapsed = 0.0\n",
        "            records.append({**prefix_base, 'backend': 'conversion', 'component': 'conversion', 'elapsed_s': elapsed, 'success': True, 'error': None})\n",
        "\n",
        "        if prefix_state is not None:\n",
        "            elapsed, success, _, error = time_call(sv_backend.run, split.tail, initial_state=prefix_state, want_statevector=True)\n",
        "            records.append({**tail_base, 'backend': 'sv', 'component': 'tail', 'elapsed_s': elapsed, 'success': success, 'error': error})\n",
        "            if dd_backend is not None:\n",
        "                elapsed, success, _, error = time_call(dd_backend.run, split.tail, initial_state=None, want_statevector=False)\n",
        "                records.append({**tail_base, 'backend': 'dd', 'component': 'tail', 'elapsed_s': elapsed, 'success': success, 'error': error})\n",
        "\n",
        "    return records\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "specs = generate_specs(qubits=[4, 6, 8], samples_per_setting=3, depth_cliff=120, tail_layers=12, angle_scale=0.3, seed=11)\n",
        "records = collect_calibration_samples(specs)\n",
        "len(records)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if pd is not None:\n",
        "    calibration_df = pd.DataFrame(records)\n",
        "    display(calibration_df.head())\n",
        "else:\n",
        "    calibration_df = records\n",
        "    calibration_df[:3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_regression_metrics(actual: np.ndarray, predicted: np.ndarray) -> Dict[str, float]:\n",
        "    mask = actual > 0\n",
        "    filtered_actual = actual[mask]\n",
        "    filtered_pred = predicted[mask]\n",
        "    if filtered_actual.size == 0:\n",
        "        return {'mape': float('nan'), 'r2': float('nan')}\n",
        "    mape = float(np.mean(np.abs((filtered_actual - filtered_pred) / filtered_actual)))\n",
        "    sse = float(np.sum((filtered_actual - filtered_pred) ** 2))\n",
        "    sst = float(np.sum((filtered_actual - np.mean(filtered_actual)) ** 2))\n",
        "    r2 = float(1.0 - sse / sst) if sst > 0 else float('nan')\n",
        "    return {'mape': mape, 'r2': r2}\n",
        "\n",
        "\n",
        "def fit_statevector(df) -> Dict[str, float]:\n",
        "    subset = df[(df['backend'] == 'sv') & df['success']]\n",
        "    amps = 2 ** subset['num_qubits']\n",
        "    amp_ops_one = amps * subset['one_qubit_gates']\n",
        "    amp_ops_two = amps * subset['two_qubit_gates']\n",
        "    X = np.column_stack([amp_ops_one, amp_ops_two])\n",
        "    y = subset['elapsed_s'].to_numpy(dtype=float)\n",
        "    coeffs, *_ = np.linalg.lstsq(X, y, rcond=None)\n",
        "    scale_one, scale_two = coeffs\n",
        "    predictions = X @ coeffs\n",
        "    metrics = compute_regression_metrics(y, predictions)\n",
        "    factor = float(scale_two / scale_one) if scale_one else float('nan')\n",
        "    return {\n",
        "        'time_scale': float(scale_one),\n",
        "        'sv_twoq_factor': factor,\n",
        "        'mape': metrics['mape'],\n",
        "        'r2': metrics['r2'],\n",
        "        'num_samples': int(len(subset)),\n",
        "        'predictions': predictions,\n",
        "        'targets': y,\n",
        "        'index': subset.index.to_numpy(),\n",
        "    }\n",
        "\n",
        "\n",
        "def fit_tableau(df) -> Dict[str, float]:\n",
        "    subset = df[(df['backend'] == 'tableau') & df['success']]\n",
        "    gate_weight = subset['one_qubit_gates'] + subset['two_qubit_gates']\n",
        "    X = gate_weight.to_numpy(dtype=float)[:, None]\n",
        "    y = subset['elapsed_s'].to_numpy(dtype=float)\n",
        "    coeffs, *_ = np.linalg.lstsq(X, y, rcond=None)\n",
        "    unit_cost = coeffs[0] if coeffs.size else float('nan')\n",
        "    predictions = (X.flatten() * unit_cost)\n",
        "    metrics = compute_regression_metrics(y, predictions)\n",
        "    return {\n",
        "        'tableau_prefix_unit_cost': float(unit_cost),\n",
        "        'mape': metrics['mape'],\n",
        "        'r2': metrics['r2'],\n",
        "        'num_samples': int(len(subset)),\n",
        "        'predictions': predictions,\n",
        "        'targets': y,\n",
        "        'index': subset.index.to_numpy(),\n",
        "    }\n",
        "\n",
        "\n",
        "def fit_conversion(df) -> Dict[str, float]:\n",
        "    subset = df[(df['backend'] == 'conversion') & df['success']]\n",
        "    amps = 2 ** subset['num_qubits']\n",
        "    X = amps.to_numpy(dtype=float)[:, None]\n",
        "    y = subset['elapsed_s'].to_numpy(dtype=float)\n",
        "    coeffs, *_ = np.linalg.lstsq(X, y, rcond=None)\n",
        "    factor = coeffs[0] if coeffs.size else float('nan')\n",
        "    predictions = (X.flatten() * factor)\n",
        "    metrics = compute_regression_metrics(y, predictions)\n",
        "    return {\n",
        "        'conv_amp_ops_factor': float(factor),\n",
        "        'mape': metrics['mape'],\n",
        "        'r2': metrics['r2'],\n",
        "        'num_samples': int(len(subset)),\n",
        "        'predictions': predictions,\n",
        "        'targets': y,\n",
        "        'index': subset.index.to_numpy(),\n",
        "    }\n",
        "\n",
        "\n",
        "def fit_decision_diagram(df) -> Dict[str, float]:\n",
        "    subset = df[(df['backend'] == 'dd') & df['success']]\n",
        "    if subset.empty:\n",
        "        return {\n",
        "            'dd_gate_node_factor': float('nan'),\n",
        "            'dd_frontier_weight': float('nan'),\n",
        "            'dd_rotation_weight': float('nan'),\n",
        "            'dd_twoq_weight': float('nan'),\n",
        "            'dd_sparsity_discount': float('nan'),\n",
        "            'dd_base_cost': float('nan'),\n",
        "            'mape': float('nan'),\n",
        "            'r2': float('nan'),\n",
        "            'num_samples': 0,\n",
        "            'predictions': np.array([]),\n",
        "            'targets': np.array([]),\n",
        "            'index': np.array([], dtype=int),\n",
        "        }\n",
        "    frontier = subset['num_qubits'].to_numpy(dtype=float)\n",
        "    num_gates = subset['num_gates'].to_numpy(dtype=float)\n",
        "    base_nodes = frontier * np.maximum(1.0, np.log2(frontier + 1.0))\n",
        "    gate_factor = np.maximum(1.0, np.log2(num_gates + 1.0))\n",
        "    M = num_gates * base_nodes * gate_factor\n",
        "    mask = M > 0\n",
        "    M = M[mask]\n",
        "    if M.size == 0:\n",
        "        return {\n",
        "            'dd_gate_node_factor': float('nan'),\n",
        "            'dd_frontier_weight': float('nan'),\n",
        "            'dd_rotation_weight': float('nan'),\n",
        "            'dd_twoq_weight': float('nan'),\n",
        "            'dd_sparsity_discount': float('nan'),\n",
        "            'dd_base_cost': float('nan'),\n",
        "            'mape': float('nan'),\n",
        "            'r2': float('nan'),\n",
        "            'num_samples': 0,\n",
        "            'predictions': np.array([]),\n",
        "            'targets': np.array([]),\n",
        "            'index': np.array([], dtype=int),\n",
        "        }\n",
        "    log_frontier = np.log2(frontier[mask] + 1.0)\n",
        "    rot_density = subset['rotation_count'].to_numpy(dtype=float)[mask] / np.maximum(1.0, num_gates[mask])\n",
        "    twoq_density = subset['two_qubit_gates'].to_numpy(dtype=float)[mask] / np.maximum(1.0, num_gates[mask])\n",
        "    sparsity = np.clip(subset['sparsity'].to_numpy(dtype=float)[mask], 0.0, 1.0)\n",
        "    X = np.column_stack([\n",
        "        M,\n",
        "        M * log_frontier,\n",
        "        M * rot_density,\n",
        "        M * twoq_density,\n",
        "        -M * sparsity,\n",
        "        np.ones_like(M),\n",
        "    ])\n",
        "    y = subset['elapsed_s'].to_numpy(dtype=float)[mask]\n",
        "    coeffs, *_ = np.linalg.lstsq(X, y, rcond=None)\n",
        "    alpha, alpha_wf, alpha_wr, alpha_wt, alpha_ws, base = coeffs\n",
        "    predictions = X @ coeffs\n",
        "    metrics = compute_regression_metrics(y, predictions)\n",
        "    gate_node_factor = float(alpha)\n",
        "    frontier_weight = float(alpha_wf / alpha) if alpha else float('nan')\n",
        "    rotation_weight = float(alpha_wr / alpha) if alpha else float('nan')\n",
        "    twoq_weight = float(alpha_wt / alpha) if alpha else float('nan')\n",
        "    sparsity_discount = float(alpha_ws / alpha) if alpha else float('nan')\n",
        "    return {\n",
        "        'dd_gate_node_factor': gate_node_factor,\n",
        "        'dd_frontier_weight': frontier_weight,\n",
        "        'dd_rotation_weight': rotation_weight,\n",
        "        'dd_twoq_weight': twoq_weight,\n",
        "        'dd_sparsity_discount': sparsity_discount,\n",
        "        'dd_base_cost': float(base),\n",
        "        'mape': metrics['mape'],\n",
        "        'r2': metrics['r2'],\n",
        "        'num_samples': int(M.size),\n",
        "        'predictions': predictions,\n",
        "        'targets': y,\n",
        "        'index': subset.index.to_numpy()[mask],\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if pd is None:\n",
        "    raise RuntimeError('Install pandas to continue the calibration workflow.')\n",
        "\n",
        "sv_fit = fit_statevector(calibration_df)\n",
        "tableau_fit = fit_tableau(calibration_df)\n",
        "conversion_fit = fit_conversion(calibration_df)\n",
        "dd_fit = fit_decision_diagram(calibration_df)\n",
        "\n",
        "model = {\n",
        "    'statevector': {key: sv_fit[key] for key in ['time_scale', 'sv_twoq_factor', 'mape', 'r2', 'num_samples']},\n",
        "    'tableau': {key: tableau_fit[key] for key in ['tableau_prefix_unit_cost', 'mape', 'r2', 'num_samples']},\n",
        "    'conversion': {key: conversion_fit[key] for key in ['conv_amp_ops_factor', 'mape', 'r2', 'num_samples']},\n",
        "    'decision_diagram': {\n",
        "        key: dd_fit[key]\n",
        "        for key in [\n",
        "            'dd_gate_node_factor',\n",
        "            'dd_frontier_weight',\n",
        "            'dd_rotation_weight',\n",
        "            'dd_twoq_weight',\n",
        "            'dd_sparsity_discount',\n",
        "            'dd_base_cost',\n",
        "            'mape',\n",
        "            'r2',\n",
        "            'num_samples',\n",
        "        ]\n",
        "    },\n",
        "}\n",
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_df = calibration_df.copy()\n",
        "pred_df['predicted_s'] = np.nan\n",
        "pred_df.loc[sv_fit['index'], 'predicted_s'] = sv_fit['predictions']\n",
        "pred_df.loc[tableau_fit['index'], 'predicted_s'] = tableau_fit['predictions']\n",
        "pred_df.loc[conversion_fit['index'], 'predicted_s'] = conversion_fit['predictions']\n",
        "pred_df.loc[dd_fit['index'], 'predicted_s'] = dd_fit['predictions']\n",
        "pred_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_dir = Path('final_results')\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "model_path = results_dir / 'cost_estimator_calibration.json'\n",
        "pred_csv = results_dir / 'estimator_calibration_predictions.csv'\n",
        "with model_path.open('w', encoding='utf-8') as handle:\n",
        "    json.dump(model, handle, indent=2)\n",
        "pred_df.to_csv(pred_csv, index=False)\n",
        "model_path, pred_csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4.5), sharey=True)\n",
        "for ax, (backend, label) in zip(axes, [('sv', 'Statevector'), ('tableau', 'Tableau prefix'), ('conversion', 'Tableau\u2192SV')]):\n",
        "    subset = pred_df[pred_df['backend'] == backend]\n",
        "    ax.scatter(subset['elapsed_s'], subset['predicted_s'])\n",
        "    ax.set_title(label)\n",
        "    ax.set_xlabel('Measured (s)')\n",
        "    ax.set_ylabel('Predicted (s)')\n",
        "    ax.plot([0, subset['elapsed_s'].max()], [0, subset['elapsed_s'].max()], 'k--', linewidth=1)\n",
        "fig.tight_layout()\n",
        "plot_path = Path('plots') / 'estimator_calibration_fit.png'\n",
        "plot_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "fig.savefig(plot_path, dpi=150)\n",
        "plot_path\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}